{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import concurrent.futures\n",
    "\n",
    "from XRootD import client\n",
    "from XRootD.client.flags import DirListFlags, StatInfoFlags, OpenFlags, MkDirFlags, QueryCode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import local classes from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils.ObjectExtractor\n",
    "%aimport utils.PlotMaker\n",
    "%aimport utils.HistogramContainer\n",
    "%aimport utils.HistogramCalculator\n",
    "OE = utils.ObjectExtractor\n",
    "PM = utils.PlotMaker\n",
    "HCont = utils.HistogramContainer\n",
    "HCalc = utils.HistogramCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=4, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "print(sys.version_info)\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# print(num_cores)\n",
    "\n",
    "\n",
    "executor = concurrent.futures.ThreadPoolExecutor(48)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['grid.linestyle'] = ':'\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "numCuts = np.arange(0,16)\n",
    "\n",
    "branch_path = 'SREffi_gbm'\n",
    "\n",
    "labels = [ f'cut{cut}' for cut in numCuts ]\n",
    "cut_descriptions = [\n",
    "    'cut1: MET/MHT trigger fired (120 GeV)',\n",
    "    'cut2: j1 pT > 120 GeV, <= 2j w/ pT > 30 GeV',\n",
    "    'cut3: mu1 pT > 5 GeV, 0.1 < |dxy| < 700 cm',\n",
    "    'cut4: mu2 pT > 5 GeV, 0.1 < |dxy| < 700 cm',\n",
    "    r'cut5: $|\\Delta\\Phi$(MET, mu pair)| < 0.4',\n",
    "    'cut6: ',\n",
    "    'cut7: ',\n",
    "    'cut8: ',\n",
    "    'cut9: ',\n",
    "    'cut10:',\n",
    "    'cut11:',\n",
    "    'cut12:',\n",
    "    'cut13:',\n",
    "    'cut14:',\n",
    "    'cut15:',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot_vars = ['metpt', 'jetpt','metjetphi', 'metmuphi', 'leadingmupt', 'subleadingmupt','recodr', 'recovertex']\n",
    "#all_plot_vars = ['reco_PF_MET_pt', 'reco_PF_jet_pt','metjetphi', 'metmuphi', 'leadingmupt', 'subleadingmupt', 'recodr', 'recovertex']\n",
    "plot_vars_metjet = all_plot_vars[0:4] #['metpt', 'jetpt', 'metjetphi', 'metmuphi']\n",
    "plot_vars_muons = all_plot_vars[4:8] #['leadingmupt', 'subleadingmupt', 'recodr', 'recovertex']\n",
    "cutflow_vars = ['cutflow_incl', 'cutflow_excl']\n",
    "all_plot_xlabels = [\n",
    "    'MET [GeV]', 'Leading jet pT [GeV]', '$\\Delta\\Phi$(MET, jet)', '$\\Delta\\Phi$(MET, di-muon)',\n",
    "    'Leading muon pT [GeV]', 'Subleading muon pT [GeV]', 'dR(muons)', 'Di-muon vertex [cm]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "histos = {}\n",
    "all_bins = {}\n",
    "for plot_var in all_plot_vars:\n",
    "    histos[plot_var] = {}\n",
    "    all_bins[plot_var] = 60\n",
    "histos['cutflow_incl'] = {}\n",
    "histos['cutflow_excl'] = {}\n",
    "histos['sumgenwgt'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Mchi-6p0_dMchi-2p0_ctau-1', 1),\n",
       " ('Mchi-6p0_dMchi-2p0_ctau-10', 1),\n",
       " ('Mchi-6p0_dMchi-2p0_ctau-100', 1),\n",
       " ('Mchi-6p0_dMchi-2p0_ctau-1000', 1),\n",
       " ('Mchi-60p0_dMchi-20p0_ctau-1', 1),\n",
       " ('Mchi-60p0_dMchi-20p0_ctau-10', 1),\n",
       " ('Mchi-60p0_dMchi-20p0_ctau-100', 1),\n",
       " ('Mchi-60p0_dMchi-20p0_ctau-1000', 1),\n",
       " ('Mchi-52p5_dMchi-5p0_ctau-1', 1),\n",
       " ('Mchi-52p5_dMchi-5p0_ctau-10', 1),\n",
       " ('Mchi-52p5_dMchi-5p0_ctau-100', 1),\n",
       " ('Mchi-52p5_dMchi-5p0_ctau-1000', 1),\n",
       " ('Mchi-5p25_dMchi-0p5_ctau-1', 1),\n",
       " ('Mchi-5p25_dMchi-0p5_ctau-10', 1),\n",
       " ('Mchi-5p25_dMchi-0p5_ctau-100', 1),\n",
       " ('Mchi-5p25_dMchi-0p5_ctau-1000', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## new signal input\n",
    "with open('config/sig.json') as sigs_json_file:\n",
    "    sigs = json.load(sigs_json_file)\n",
    "\n",
    "xrdfs = client.FileSystem(\"root://cmseos.fnal.gov/\")\n",
    "\n",
    "redirector = 'root://cmsxrootd.fnal.gov'\n",
    "sig_base_dir = '/store/group/lpcmetx/iDM/Ntuples/2018/signal/'\n",
    "files = {}\n",
    "\n",
    "for sig, properties in sigs.items():\n",
    "    files[sig] = []\n",
    "    status, listing = xrdfs.dirlist(f'{sig_base_dir}/{properties[\"dir\"]}', DirListFlags.STAT)\n",
    "    for file in listing:\n",
    "        if '.root' in file.name:\n",
    "            files[sig].append(f'{redirector}/{sig_base_dir}/{properties[\"dir\"]}/{file.name}')\n",
    "num_files_total = np.sum(np.array([len(files[i]) for i in files]))\n",
    "print(num_files_total)\n",
    "[(i, len(files[i])) for i in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing signal Mchi-6p0_dMchi-2p0_ctau-1 (1/16)\n",
      "Reading file 1 of 1, global 1 of 16 (0.00%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-6p0_dMchi-2p0_ctau-10 (2/16)\n",
      "Reading file 1 of 1, global 2 of 16 (6.25%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-6p0_dMchi-2p0_ctau-100 (3/16)\n",
      "Reading file 1 of 1, global 3 of 16 (12.50%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-6p0_dMchi-2p0_ctau-1000 (4/16)\n",
      "Reading file 1 of 1, global 4 of 16 (18.75%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-60p0_dMchi-20p0_ctau-1 (5/16)\n",
      "Reading file 1 of 1, global 5 of 16 (25.00%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-60p0_dMchi-20p0_ctau-10 (6/16)\n",
      "Reading file 1 of 1, global 6 of 16 (31.25%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-60p0_dMchi-20p0_ctau-100 (7/16)\n",
      "Reading file 1 of 1, global 7 of 16 (37.50%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-60p0_dMchi-20p0_ctau-1000 (8/16)\n",
      "Reading file 1 of 1, global 8 of 16 (43.75%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-52p5_dMchi-5p0_ctau-1 (9/16)\n",
      "Reading file 1 of 1, global 9 of 16 (50.00%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-52p5_dMchi-5p0_ctau-10 (10/16)\n",
      "Reading file 1 of 1, global 10 of 16 (56.25%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-52p5_dMchi-5p0_ctau-100 (11/16)\n",
      "Reading file 1 of 1, global 11 of 16 (62.50%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-52p5_dMchi-5p0_ctau-1000 (12/16)\n",
      "Reading file 1 of 1, global 12 of 16 (68.75%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-5p25_dMchi-0p5_ctau-1 (13/16)\n",
      "Reading file 1 of 1, global 13 of 16 (75.00%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-5p25_dMchi-0p5_ctau-10 (14/16)\n",
      "Reading file 1 of 1, global 14 of 16 (81.25%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-5p25_dMchi-0p5_ctau-100 (15/16)\n",
      "Reading file 1 of 1, global 15 of 16 (87.50%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "Processing signal Mchi-5p25_dMchi-0p5_ctau-1000 (16/16)\n",
      "Reading file 1 of 1, global 16 of 16 (93.75%)\n",
      "Sample \"\" does not have either pileup or weight information\n",
      "CPU times: user 3min 44s, sys: 43.5 s, total: 4min 27s\n",
      "Wall time: 6min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MAX_FILES=None # To load all possible files\n",
    "# MAX_FILES=1 # For testing\n",
    "\n",
    "### Initialize empty dicts of histograms \n",
    "# histos = {}\n",
    "# all_bins = {}\n",
    "# for plot_var in all_plot_vars:\n",
    "#     histos[plot_var] = {}\n",
    "#     all_bins[plot_var] = 60\n",
    "# histos['cutflow_incl'] = {}\n",
    "# histos['cutflow_excl'] = {}\n",
    "# histos['sumgenwgt'] = {}\n",
    "\n",
    "global_file_counter = 1\n",
    "\n",
    "for sig in sigs:\n",
    "    \n",
    "    print(f'Processing signal {sig} ({(list(sigs.keys())).index(sig)+1}/{len(sigs)})')\n",
    "    \n",
    "    ### Initialize histograms as empty HistogramContainers\n",
    "    for plot_var in all_plot_vars:\n",
    "        histos[plot_var][sig] = HCont.HistogramContainer(all_bins[plot_var])\n",
    "    histos['cutflow_incl'][sig] = np.zeros(len(numCuts))\n",
    "    histos['cutflow_excl'][sig] = np.zeros(len(numCuts))\n",
    "    histos['sumgenwgt'][sig] = 0.0\n",
    "    \n",
    "    ### Load data\n",
    "    file_counter = 1\n",
    "    for file in files[sig][slice(0,MAX_FILES)]:\n",
    "        \n",
    "        if file_counter % 10 == 1:\n",
    "            print(f'Reading file {file_counter} of {len(files[sig])},'\n",
    "                  f' global {global_file_counter} of {num_files_total}'\n",
    "                  f' ({100*(global_file_counter-1)/num_files_total:.2f}%)')\n",
    "            with open('histos_temp.dat', 'wb') as histos_file:\n",
    "                pickle.dump(histos, histos_file)\n",
    "        file_counter += 1\n",
    "        global_file_counter += 1\n",
    "        \n",
    "        ### Open ROOT file and get tree\n",
    "        tree = uproot.open(file)[branch_path + '/reco']\n",
    "        \n",
    "        ### Make pandas dataframes and create all objects that will be passed to histo functions\n",
    "        obj_extractor = OE.ObjectExtractor(tree)\n",
    "        objects = obj_extractor.get_all()\n",
    "            \n",
    "        ## Add to sum of genwgts\n",
    "        histos['sumgenwgt'][sig] += np.sum(objects['gen_wgt'])\n",
    "        \n",
    "        ### Calculate histograms and cutflows\n",
    "        histo_maker = HCalc.HistogramCalculator(objects, sig)\n",
    "            \n",
    "        ### Cutflows\n",
    "        incl, excl = histo_maker.cutflows()\n",
    "        histos['cutflow_incl'][sig] += incl\n",
    "        histos['cutflow_excl'][sig] += excl\n",
    "        \n",
    "        ### Histograms\n",
    "        for plot_var in all_plot_vars:\n",
    "            new_hist = eval(f'histo_maker.{plot_var}()')\n",
    "            histos[plot_var][sig] += new_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mchi-6p0_dMchi-2p0_ctau-1 7053.7400881057265 45400.0\n",
      "Mchi-6p0_dMchi-2p0_ctau-10 7057.159857199524 45378.0\n",
      "Mchi-6p0_dMchi-2p0_ctau-100 7053.7400881057265 45400.0\n",
      "Mchi-6p0_dMchi-2p0_ctau-1000 7053.7400881057265 45400.0\n",
      "Mchi-60p0_dMchi-20p0_ctau-1 2279.205722216291 140505.0\n",
      "Mchi-60p0_dMchi-20p0_ctau-10 2278.4759871931697 140550.0\n",
      "Mchi-60p0_dMchi-20p0_ctau-100 2278.897554866073 140524.0\n",
      "Mchi-60p0_dMchi-20p0_ctau-1000 2276.5646771120655 140668.0\n",
      "Mchi-52p5_dMchi-5p0_ctau-1 2031.5401499676466 157634.0\n",
      "Mchi-52p5_dMchi-5p0_ctau-10 2033.2556618688136 157501.0\n",
      "Mchi-52p5_dMchi-5p0_ctau-100 2040.497763504989 156942.0\n",
      "Mchi-52p5_dMchi-5p0_ctau-1000 2036.4363613239643 157255.0\n",
      "Mchi-5p25_dMchi-0p5_ctau-1 5487.034593835136 58363.0\n",
      "Mchi-5p25_dMchi-0p5_ctau-10 5491.174402853272 58319.0\n",
      "Mchi-5p25_dMchi-0p5_ctau-100 5485.436793422405 58380.0\n",
      "Mchi-5p25_dMchi-0p5_ctau-1000 5485.436793422405 58380.0\n"
     ]
    }
   ],
   "source": [
    "luminosity = 59.97 * 1000 # 1/pb\n",
    "for sig, properties in sigs.items():\n",
    "    properties['weight'] = luminosity * properties['xsec'] / histos['sumgenwgt'][sig]\n",
    "#     except KeyError:\n",
    "#         properties['weight'] = 1\n",
    "for sig, properties in sigs.items():\n",
    "    try:\n",
    "        print(sig, luminosity * properties['xsec'] / histos['sumgenwgt'][sig], histos['sumgenwgt'][sig])\n",
    "    except KeyError: pass\n",
    "    \n",
    "with open('histos_signal_objects_gbm.dat', 'wb') as histos_file:\n",
    "    pickle.dump(histos, histos_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/bkgs2.json') as bkgs_json_file:\n",
    "    bkgs = json.load(bkgs_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrdfs = client.FileSystem(\"root://cmseos.fnal.gov/\")\n",
    "\n",
    "redirector = 'root://cmsxrootd.fnal.gov'\n",
    "bkg_base_dir = '/store/group/lpcmetx/iDM/Ntuples/2018/backgrounds'\n",
    "files = {}\n",
    "\n",
    "for bkg, properties in bkgs.items():\n",
    "    files[bkg] = []\n",
    "    status, listing = xrdfs.dirlist(f'{bkg_base_dir}/{properties[\"dir\"]}', DirListFlags.STAT)\n",
    "    for file in listing:\n",
    "        if '.root' in file.name:\n",
    "            files[bkg].append(f'{redirector}/{bkg_base_dir}/{properties[\"dir\"]}/{file.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files_total = np.sum(np.array([len(files[i]) for i in files]))\n",
    "print(num_files_total)\n",
    "[(i, len(files[i])) for i in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MAX_FILES=None # To load all possible files\n",
    "# MAX_FILES=1 # For testing\n",
    "\n",
    "### Initialize empty dicts of histograms \n",
    "# histos = {}\n",
    "# all_bins = {}\n",
    "# for plot_var in all_plot_vars:\n",
    "#     histos[plot_var] = {}\n",
    "#     all_bins[plot_var] = 60\n",
    "# histos['cutflow_incl'] = {}\n",
    "# histos['cutflow_excl'] = {}\n",
    "# histos['sumgenwgt'] = {}\n",
    "\n",
    "global_file_counter = 1\n",
    "\n",
    "for bkg in bkgs:\n",
    "    \n",
    "    print(f'Processing background {bkg} ({(list(bkgs.keys())).index(bkg)+1}/{len(bkgs)})')\n",
    "    \n",
    "    ### Initialize histograms as empty HistogramContainers\n",
    "    for plot_var in all_plot_vars:\n",
    "        histos[plot_var][bkg] = HCont.HistogramContainer(all_bins[plot_var])\n",
    "    histos['cutflow_incl'][bkg] = np.zeros(len(numCuts))\n",
    "    histos['cutflow_excl'][bkg] = np.zeros(len(numCuts))\n",
    "    histos['sumgenwgt'][bkg] = 0.0\n",
    "    \n",
    "    ### Load data\n",
    "    file_counter = 1\n",
    "    for file in files[bkg][slice(0,MAX_FILES)]:\n",
    "        \n",
    "        if file_counter % 10 == 1:\n",
    "            print(f'Reading file {file_counter} of {len(files[bkg])},'\n",
    "                  f' global {global_file_counter} of {num_files_total}'\n",
    "                  f' ({100*(global_file_counter-1)/num_files_total:.2f}%)')\n",
    "            with open('histos_temp.dat', 'wb') as histos_file:\n",
    "                pickle.dump(histos, histos_file)\n",
    "        file_counter += 1\n",
    "        global_file_counter += 1\n",
    "        \n",
    "        ### Open ROOT file and get tree\n",
    "        tree = uproot.open(file)[branch_path + '/cutsTree']\n",
    "        \n",
    "        ### Make pandas dataframes and create all objects that will be passed to histo functions\n",
    "        obj_extractor = OE.ObjectExtractor(tree)\n",
    "        objects = obj_extractor.get_all()\n",
    "            \n",
    "        ## Add to sum of genwgts\n",
    "        histos['sumgenwgt'][bkg] += np.sum(objects['genwgt'])\n",
    "        \n",
    "        ### Calculate histograms and cutflows\n",
    "        histo_maker = HCalc.HistogramCalculator(objects, bkg)\n",
    "            \n",
    "        ### Cutflows\n",
    "        incl, excl = histo_maker.cutflows()\n",
    "        histos['cutflow_incl'][bkg] += incl\n",
    "        histos['cutflow_excl'][bkg] += excl\n",
    "        \n",
    "        ### Histograms\n",
    "        for plot_var in all_plot_vars:\n",
    "            new_hist = eval(f'histo_maker.{plot_var}()')\n",
    "            histos[plot_var][bkg] += new_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luminosity = 59.97 * 1000 # 1/pb\n",
    "for bkg, properties in bkgs.items():\n",
    "    properties['weight'] = luminosity * properties['xsec'] / histos['sumgenwgt'][bkg]\n",
    "#     except KeyError:\n",
    "#         properties['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bkg, properties in bkgs.items():\n",
    "    try:\n",
    "        print(bkg, luminosity * properties['xsec'] / histos['sumgenwgt'][bkg], histos['sumgenwgt'][bkg])\n",
    "    except KeyError: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('histos_bkgs_objects_gbm.dat', 'wb') as histos_file:\n",
    "    pickle.dump(histos, histos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cutFlowInclGrp = {}\n",
    "for grp in bkg_grps:\n",
    "    if '60p0' in grp or '5p25' in grp or '52p5' in grp or '6p0' in grp: continue\n",
    "    for bkg in bkg_grps[grp]:\n",
    "        if grp in cutFlowInclGrp.keys():\n",
    "            cutFlowInclGrp[grp] += histos['cutflow_incl'][bkg].astype(int)\n",
    "        else:\n",
    "            cutFlowInclGrp[grp] = histos['cutflow_incl'][bkg].astype(int)\n",
    "\n",
    "pd.DataFrame.from_dict(cutFlowInclGrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutFlowInclGrp2 = {}\n",
    "for grp in bkg_grps:\n",
    "#     if '60p0' in grp or '5p25' in grp or '52p5' in grp or '6p0' in grp: continue\n",
    "    for bkg in bkg_grps[grp]:\n",
    "        if grp in cutFlowInclGrp2.keys():\n",
    "            try:\n",
    "                cutFlowInclGrp2[grp] += (histos['cutflow_incl'][bkg]*bkgs[bkg]['weight']).astype(int)\n",
    "            except KeyError: pass\n",
    "        else:\n",
    "            try:\n",
    "                cutFlowInclGrp2[grp] = (histos['cutflow_incl'][bkg]*bkgs[bkg]['weight']).astype(int)\n",
    "            except KeyError: pass\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(cutFlowInclGrp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
