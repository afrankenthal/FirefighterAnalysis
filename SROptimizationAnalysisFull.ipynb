{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import concurrent.futures\n",
    "\n",
    "from XRootD import client\n",
    "from XRootD.client.flags import DirListFlags, StatInfoFlags, OpenFlags, MkDirFlags, QueryCode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import local classes from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils.ObjectExtractor\n",
    "%aimport utils.PlotMaker\n",
    "%aimport utils.HistogramContainer\n",
    "%aimport utils.HistogramCalculator\n",
    "OE = utils.ObjectExtractor\n",
    "PM = utils.PlotMaker\n",
    "HCont = utils.HistogramContainer\n",
    "HCalc = utils.HistogramCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=4, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "print(sys.version_info)\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# print(num_cores)\n",
    "\n",
    "\n",
    "executor = concurrent.futures.ThreadPoolExecutor(48)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['grid.linestyle'] = ':'\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "numCuts = np.arange(0,6)\n",
    "\n",
    "branch_path = 'SREffi_gbm'\n",
    "\n",
    "labels = [ f'cut{cut}' for cut in numCuts ]\n",
    "cut_descriptions = [\n",
    "    'cut1: MET/MHT trigger fired (120 GeV)',\n",
    "    'cut2: j1 pT > 120 GeV, <= 2j w/ pT > 30 GeV',\n",
    "    'cut3: mu1 pT > 5 GeV, 0.1 < |dxy| < 700 cm',\n",
    "    'cut4: mu2 pT > 5 GeV, 0.1 < |dxy| < 700 cm',\n",
    "    r'cut5: $|\\Delta\\Phi$(MET, mu pair)| < 0.4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_plot_vars = ['metpt', 'jetpt','metjetphi', 'metmuphi', 'leadingmupt', 'subleadingmupt', 'recodr', 'recovertex']\n",
    "plot_vars_metjet = all_plot_vars[0:4] #['metpt', 'jetpt', 'metjetphi', 'metmuphi']\n",
    "plot_vars_muons = all_plot_vars[4:8] #['leadingmupt', 'subleadingmupt', 'recodr', 'recovertex']\n",
    "cutflow_vars = ['cutflow_incl', 'cutflow_excl']\n",
    "all_plot_xlabels = [\n",
    "    'MET [GeV]', 'Leading jet pT [GeV]', '$\\Delta\\Phi$(MET, jet)', '$\\Delta\\Phi$(MET, di-muon)',\n",
    "    'Leading muon pT [GeV]', 'Subleading muon pT [GeV]', 'dR(muons)', 'Di-muon vertex [cm]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histos = {}\n",
    "all_bins = {}\n",
    "for plot_var in all_plot_vars:\n",
    "    histos[plot_var] = {}\n",
    "    all_bins[plot_var] = 60\n",
    "histos['cutflow_incl'] = {}\n",
    "histos['cutflow_excl'] = {}\n",
    "histos['sumgenwgt'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Mchi-60p0_dMchi-20p0_ctau-1', 229), ('Mchi-5p25_dMchi-0p5_ctau-1000', 495)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## new signal input\n",
    "with open('config/sig.json') as sigs_json_file:\n",
    "    sigs = json.load(sigs_json_file)\n",
    "\n",
    "xrdfs = client.FileSystem(\"root://cmseos.fnal.gov/\")\n",
    "\n",
    "redirector = 'root://cmsxrootd.fnal.gov'\n",
    "sig_base_dir = '/store/group/lpcmetx/iDM/Ntuples/2018/signal/track_quality/iDM_2018_MC'\n",
    "files = {}\n",
    "\n",
    "for sig, properties in sigs.items():\n",
    "    files[sig] = []\n",
    "    status, listing = xrdfs.dirlist(f'{sig_base_dir}/{properties[\"dir\"]}', DirListFlags.STAT)\n",
    "    for file in listing:\n",
    "        if '.root' in file.name:\n",
    "            files[sig].append(f'{redirector}/{sig_base_dir}/{properties[\"dir\"]}/{file.name}')\n",
    "num_files_total = np.sum(np.array([len(files[i]) for i in files]))\n",
    "print(num_files_total)\n",
    "[(i, len(files[i])) for i in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing signal Mchi-60p0_dMchi-20p0_ctau-1 (1/2)\n",
      "Reading file 1 of 229, global 1 of 724 (0.00%)\n",
      "Reading file 11 of 229, global 11 of 724 (1.38%)\n",
      "Reading file 21 of 229, global 21 of 724 (2.76%)\n",
      "Reading file 31 of 229, global 31 of 724 (4.14%)\n",
      "Reading file 41 of 229, global 41 of 724 (5.52%)\n",
      "Reading file 51 of 229, global 51 of 724 (6.91%)\n",
      "Reading file 61 of 229, global 61 of 724 (8.29%)\n",
      "Reading file 71 of 229, global 71 of 724 (9.67%)\n",
      "Reading file 81 of 229, global 81 of 724 (11.05%)\n",
      "Reading file 91 of 229, global 91 of 724 (12.43%)\n",
      "Reading file 101 of 229, global 101 of 724 (13.81%)\n",
      "Reading file 111 of 229, global 111 of 724 (15.19%)\n",
      "Reading file 121 of 229, global 121 of 724 (16.57%)\n",
      "Reading file 131 of 229, global 131 of 724 (17.96%)\n",
      "Reading file 141 of 229, global 141 of 724 (19.34%)\n",
      "Reading file 151 of 229, global 151 of 724 (20.72%)\n",
      "Reading file 161 of 229, global 161 of 724 (22.10%)\n",
      "Reading file 171 of 229, global 171 of 724 (23.48%)\n",
      "Reading file 181 of 229, global 181 of 724 (24.86%)\n",
      "Reading file 191 of 229, global 191 of 724 (26.24%)\n",
      "Reading file 201 of 229, global 201 of 724 (27.62%)\n",
      "Reading file 211 of 229, global 211 of 724 (29.01%)\n",
      "Reading file 221 of 229, global 221 of 724 (30.39%)\n",
      "Processing signal Mchi-5p25_dMchi-0p5_ctau-1000 (2/2)\n",
      "Reading file 1 of 495, global 230 of 724 (31.63%)\n",
      "Reading file 11 of 495, global 240 of 724 (33.01%)\n",
      "Reading file 21 of 495, global 250 of 724 (34.39%)\n",
      "Reading file 31 of 495, global 260 of 724 (35.77%)\n",
      "Reading file 41 of 495, global 270 of 724 (37.15%)\n",
      "Reading file 51 of 495, global 280 of 724 (38.54%)\n",
      "Reading file 61 of 495, global 290 of 724 (39.92%)\n",
      "Reading file 71 of 495, global 300 of 724 (41.30%)\n",
      "Reading file 81 of 495, global 310 of 724 (42.68%)\n",
      "Reading file 91 of 495, global 320 of 724 (44.06%)\n",
      "Reading file 101 of 495, global 330 of 724 (45.44%)\n",
      "Reading file 111 of 495, global 340 of 724 (46.82%)\n",
      "Reading file 121 of 495, global 350 of 724 (48.20%)\n",
      "Reading file 131 of 495, global 360 of 724 (49.59%)\n",
      "Reading file 141 of 495, global 370 of 724 (50.97%)\n",
      "Reading file 151 of 495, global 380 of 724 (52.35%)\n",
      "Reading file 161 of 495, global 390 of 724 (53.73%)\n",
      "Reading file 171 of 495, global 400 of 724 (55.11%)\n",
      "Reading file 181 of 495, global 410 of 724 (56.49%)\n",
      "Reading file 191 of 495, global 420 of 724 (57.87%)\n",
      "Reading file 201 of 495, global 430 of 724 (59.25%)\n",
      "Reading file 211 of 495, global 440 of 724 (60.64%)\n",
      "Reading file 221 of 495, global 450 of 724 (62.02%)\n",
      "Reading file 231 of 495, global 460 of 724 (63.40%)\n",
      "Reading file 241 of 495, global 470 of 724 (64.78%)\n",
      "Reading file 251 of 495, global 480 of 724 (66.16%)\n",
      "Reading file 261 of 495, global 490 of 724 (67.54%)\n",
      "Reading file 271 of 495, global 500 of 724 (68.92%)\n",
      "Reading file 281 of 495, global 510 of 724 (70.30%)\n",
      "Reading file 291 of 495, global 520 of 724 (71.69%)\n",
      "Reading file 301 of 495, global 530 of 724 (73.07%)\n",
      "Reading file 311 of 495, global 540 of 724 (74.45%)\n",
      "Reading file 321 of 495, global 550 of 724 (75.83%)\n",
      "Reading file 331 of 495, global 560 of 724 (77.21%)\n",
      "Reading file 341 of 495, global 570 of 724 (78.59%)\n",
      "Reading file 351 of 495, global 580 of 724 (79.97%)\n",
      "Reading file 361 of 495, global 590 of 724 (81.35%)\n",
      "Reading file 371 of 495, global 600 of 724 (82.73%)\n",
      "Reading file 381 of 495, global 610 of 724 (84.12%)\n",
      "Reading file 391 of 495, global 620 of 724 (85.50%)\n",
      "Reading file 401 of 495, global 630 of 724 (86.88%)\n",
      "Reading file 411 of 495, global 640 of 724 (88.26%)\n",
      "Reading file 421 of 495, global 650 of 724 (89.64%)\n",
      "Reading file 431 of 495, global 660 of 724 (91.02%)\n",
      "Reading file 441 of 495, global 670 of 724 (92.40%)\n",
      "Reading file 451 of 495, global 680 of 724 (93.78%)\n",
      "Reading file 461 of 495, global 690 of 724 (95.17%)\n",
      "Reading file 471 of 495, global 700 of 724 (96.55%)\n",
      "Reading file 481 of 495, global 710 of 724 (97.93%)\n",
      "Reading file 491 of 495, global 720 of 724 (99.31%)\n",
      "CPU times: user 24min 50s, sys: 51.9 s, total: 25min 42s\n",
      "Wall time: 28min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MAX_FILES=None # To load all possible files\n",
    "# MAX_FILES=1 # For testing\n",
    "\n",
    "### Initialize empty dicts of histograms \n",
    "# histos = {}\n",
    "# all_bins = {}\n",
    "# for plot_var in all_plot_vars:\n",
    "#     histos[plot_var] = {}\n",
    "#     all_bins[plot_var] = 60\n",
    "# histos['cutflow_incl'] = {}\n",
    "# histos['cutflow_excl'] = {}\n",
    "# histos['sumgenwgt'] = {}\n",
    "\n",
    "global_file_counter = 1\n",
    "\n",
    "for sig in sigs:\n",
    "    \n",
    "    print(f'Processing signal {sig} ({(list(sigs.keys())).index(sig)+1}/{len(sigs)})')\n",
    "    \n",
    "    ### Initialize histograms as empty HistogramContainers\n",
    "    for plot_var in all_plot_vars:\n",
    "        histos[plot_var][sig] = HCont.HistogramContainer(all_bins[plot_var])\n",
    "    histos['cutflow_incl'][sig] = np.zeros(len(numCuts))\n",
    "    histos['cutflow_excl'][sig] = np.zeros(len(numCuts))\n",
    "    histos['sumgenwgt'][sig] = 0.0\n",
    "    \n",
    "    ### Load data\n",
    "    file_counter = 1\n",
    "    for file in files[sig][slice(0,MAX_FILES)]:\n",
    "        \n",
    "        if file_counter % 10 == 1:\n",
    "            print(f'Reading file {file_counter} of {len(files[sig])},'\n",
    "                  f' global {global_file_counter} of {num_files_total}'\n",
    "                  f' ({100*(global_file_counter-1)/num_files_total:.2f}%)')\n",
    "            with open('histos_temp.dat', 'wb') as histos_file:\n",
    "                pickle.dump(histos, histos_file)\n",
    "        file_counter += 1\n",
    "        global_file_counter += 1\n",
    "        \n",
    "        ### Open ROOT file and get tree\n",
    "        tree = uproot.open(file)[branch_path + '/cutsTree']\n",
    "        \n",
    "        ### Make pandas dataframes and create all objects that will be passed to histo functions\n",
    "        obj_extractor = OE.ObjectExtractor(tree)\n",
    "        objects = obj_extractor.get_all()\n",
    "            \n",
    "        ## Add to sum of genwgts\n",
    "        histos['sumgenwgt'][sig] += np.sum(objects['genwgt'])\n",
    "        \n",
    "        ### Calculate histograms and cutflows\n",
    "        histo_maker = HCalc.HistogramCalculator(objects, sig)\n",
    "            \n",
    "        ### Cutflows\n",
    "        incl, excl = histo_maker.cutflows()\n",
    "        histos['cutflow_incl'][sig] += incl\n",
    "        histos['cutflow_excl'][sig] += excl\n",
    "        \n",
    "        ### Histograms\n",
    "        for plot_var in all_plot_vars:\n",
    "            new_hist = eval(f'histo_maker.{plot_var}()')\n",
    "            histos[plot_var][sig] += new_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mchi-60p0_dMchi-20p0_ctau-1 5241.240589198036 61100.0\n",
      "Mchi-5p25_dMchi-0p5_ctau-1000 5135.092923688726 62363.0\n"
     ]
    }
   ],
   "source": [
    "luminosity = 59.97 * 1000 # 1/pb\n",
    "for sig, properties in sigs.items():\n",
    "    properties['weight'] = luminosity * properties['xsec'] / histos['sumgenwgt'][sig]\n",
    "#     except KeyError:\n",
    "#         properties['weight'] = 1\n",
    "for sig, properties in sigs.items():\n",
    "    try:\n",
    "        print(sig, luminosity * properties['xsec'] / histos['sumgenwgt'][sig], histos['sumgenwgt'][sig])\n",
    "    except KeyError: pass\n",
    "    \n",
    "with open('histos_signal_objects_gbm.dat', 'wb') as histos_file:\n",
    "    pickle.dump(histos, histos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#masses = [('60p0','20p0'),('6p0','2p0'),('52p5','5p0'),('5p25','0p5')]\n",
    "#def print_masses(mass):\n",
    "#    return f'({float(mass[0].replace(\"p\",\".\"))-float(mass[1].replace(\"p\",\".\"))/2}, ' + \\\n",
    "#           f'{float(mass[0].replace(\"p\",\".\"))+float(mass[1].replace(\"p\",\".\"))/2}) GeV'\n",
    "#    \n",
    "#mchis = dict([(mass[0], print_masses(mass)) for mass in masses])\n",
    "#ctaus = [10]#, 10, 100, 1000]\n",
    "#\n",
    "#\n",
    "#base_dir = '../Firefighter/washAOD/SROptimization/'\n",
    "#def filename(Mchi, dMchi, ctau): \n",
    "#    return base_dir + f'Mchi-{Mchi}_dMchi-{dMchi}_ctau-{ctau}.root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trees_gbm = dict()\n",
    "#gen_info_gbm = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#num_params = len(masses)*len(ctaus)\n",
    "#count_param = 1\n",
    "#\n",
    "#for (Mchi, dMchi) in masses:\n",
    "#    trees_gbm[Mchi] = dict()\n",
    "#    gen_info_gbm[Mchi] = dict()\n",
    "#    for ctau in ctaus:\n",
    "#        gen_info_gbm[Mchi][ctau] = uproot.open(filename(Mchi, dMchi, ctau))['GEN/gen']#.pandas.df(flatten=False)\n",
    "#        trees_gbm[Mchi][ctau] = uproot.open(filename(Mchi, dMchi, ctau))[branch_path + f'/cutsTree']#.pandas.df(flatten=False)\n",
    "#        print(f'{count_param} of {num_params}: ' + filename(Mchi, dMchi, ctau))\n",
    "#        count_param += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#histos_signal = {}\n",
    "#for plot_var in all_plot_vars:\n",
    "#    histos_signal[plot_var] = {}\n",
    "#for plot_var in cutflow_vars:\n",
    "#    histos_signal[plot_var] = {}\n",
    "#    \n",
    "#for mchi in mchis:\n",
    "#    for plot_var in all_plot_vars:\n",
    "#        histos_signal[plot_var][mchi] = HCont.HistogramContainer(all_bins[plot_var])\n",
    "#    histos_signal['cutflow_incl'][mchi] = np.zeros(len(numCuts))\n",
    "#    histos_signal['cutflow_excl'][mchi] = np.zeros(len(numCuts))\n",
    "#        \n",
    "#    ### Make pandas dataframes and create all objects that will be passed to histo functions\n",
    "#    obj_extractor = OE.ObjectExtractor(trees_gbm[mchi][ctau], mchi)\n",
    "#    objects = obj_extractor.get_all()\n",
    "#\n",
    "#    ### Calculate histograms and cutflows\n",
    "#    histo_computer = HCalc.HistogramCalculator(objects, mchi)\n",
    "#\n",
    "#    ### Cutflows\n",
    "#    incl, excl = histo_computer.cutflows()\n",
    "#    histos_signal['cutflow_incl'][mchi] += incl\n",
    "#    histos_signal['cutflow_excl'][mchi] += excl\n",
    "#\n",
    "#    ### Histograms\n",
    "#    for plot_var in all_plot_vars:\n",
    "#        new_hist = eval(f'histo_computer.{plot_var}()')\n",
    "#        histos_signal[plot_var][mchi] += new_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('histos_signal_objects_gbm.dat', 'wb') as histos_file:\n",
    "#    pickle.dump(histos_signal, histos_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('config/bkgs3.json') as bkgs_json_file:\n",
    "    bkgs = json.load(bkgs_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrdfs = client.FileSystem(\"root://cmseos.fnal.gov/\")\n",
    "\n",
    "redirector = 'root://cmsxrootd.fnal.gov'\n",
    "bkg_base_dir = '/store/group/lpcmetx/iDM/Ntuples/2018/backgrounds'\n",
    "files = {}\n",
    "\n",
    "for bkg, properties in bkgs.items():\n",
    "    files[bkg] = []\n",
    "    status, listing = xrdfs.dirlist(f'{bkg_base_dir}/{properties[\"dir\"]}', DirListFlags.STAT)\n",
    "    for file in listing:\n",
    "        if '.root' in file.name:\n",
    "            files[bkg].append(f'{redirector}/{bkg_base_dir}/{properties[\"dir\"]}/{file.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_files_total = np.sum(np.array([len(files[i]) for i in files]))\n",
    "print(num_files_total)\n",
    "[(i, len(files[i])) for i in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MAX_FILES=None # To load all possible files\n",
    "# MAX_FILES=1 # For testing\n",
    "\n",
    "### Initialize empty dicts of histograms \n",
    "# histos = {}\n",
    "# all_bins = {}\n",
    "# for plot_var in all_plot_vars:\n",
    "#     histos[plot_var] = {}\n",
    "#     all_bins[plot_var] = 60\n",
    "# histos['cutflow_incl'] = {}\n",
    "# histos['cutflow_excl'] = {}\n",
    "# histos['sumgenwgt'] = {}\n",
    "\n",
    "global_file_counter = 1\n",
    "\n",
    "for bkg in bkgs:\n",
    "    \n",
    "    print(f'Processing background {bkg} ({(list(bkgs.keys())).index(bkg)+1}/{len(bkgs)})')\n",
    "    \n",
    "    ### Initialize histograms as empty HistogramContainers\n",
    "    for plot_var in all_plot_vars:\n",
    "        histos[plot_var][bkg] = HCont.HistogramContainer(all_bins[plot_var])\n",
    "    histos['cutflow_incl'][bkg] = np.zeros(len(numCuts))\n",
    "    histos['cutflow_excl'][bkg] = np.zeros(len(numCuts))\n",
    "    histos['sumgenwgt'][bkg] = 0.0\n",
    "    \n",
    "    ### Load data\n",
    "    file_counter = 1\n",
    "    for file in files[bkg][slice(0,MAX_FILES)]:\n",
    "        \n",
    "        if file_counter % 10 == 1:\n",
    "            print(f'Reading file {file_counter} of {len(files[bkg])},'\n",
    "                  f' global {global_file_counter} of {num_files_total}'\n",
    "                  f' ({100*(global_file_counter-1)/num_files_total:.2f}%)')\n",
    "            with open('histos_temp.dat', 'wb') as histos_file:\n",
    "                pickle.dump(histos, histos_file)\n",
    "        file_counter += 1\n",
    "        global_file_counter += 1\n",
    "        \n",
    "        ### Open ROOT file and get tree\n",
    "        tree = uproot.open(file)[branch_path + '/cutsTree']\n",
    "        \n",
    "        ### Make pandas dataframes and create all objects that will be passed to histo functions\n",
    "        obj_extractor = OE.ObjectExtractor(tree)\n",
    "        objects = obj_extractor.get_all()\n",
    "            \n",
    "        ## Add to sum of genwgts\n",
    "        histos['sumgenwgt'][bkg] += np.sum(objects['genwgt'])\n",
    "        \n",
    "        ### Calculate histograms and cutflows\n",
    "        histo_maker = HCalc.HistogramCalculator(objects, bkg)\n",
    "            \n",
    "        ### Cutflows\n",
    "        incl, excl = histo_maker.cutflows()\n",
    "        histos['cutflow_incl'][bkg] += incl\n",
    "        histos['cutflow_excl'][bkg] += excl\n",
    "        \n",
    "        ### Histograms\n",
    "        for plot_var in all_plot_vars:\n",
    "            new_hist = eval(f'histo_maker.{plot_var}()')\n",
    "            histos[plot_var][bkg] += new_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "luminosity = 59.97 * 1000 # 1/pb\n",
    "for bkg, properties in bkgs.items():\n",
    "    properties['weight'] = luminosity * properties['xsec'] / histos['sumgenwgt'][bkg]\n",
    "#     except KeyError:\n",
    "#         properties['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for bkg, properties in bkgs.items():\n",
    "    try:\n",
    "        print(bkg, luminosity * properties['xsec'] / histos['sumgenwgt'][bkg], histos['sumgenwgt'][bkg])\n",
    "    except KeyError: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('histos_bkgs_objects_gbm.dat', 'wb') as histos_file:\n",
    "    pickle.dump(histos, histos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cutFlowInclGrp = {}\n",
    "for grp in bkg_grps:\n",
    "    if '60p0' in grp or '5p25' in grp or '52p5' in grp or '6p0' in grp: continue\n",
    "    for bkg in bkg_grps[grp]:\n",
    "        if grp in cutFlowInclGrp.keys():\n",
    "            cutFlowInclGrp[grp] += histos['cutflow_incl'][bkg].astype(int)\n",
    "        else:\n",
    "            cutFlowInclGrp[grp] = histos['cutflow_incl'][bkg].astype(int)\n",
    "\n",
    "pd.DataFrame.from_dict(cutFlowInclGrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutFlowInclGrp2 = {}\n",
    "for grp in bkg_grps:\n",
    "#     if '60p0' in grp or '5p25' in grp or '52p5' in grp or '6p0' in grp: continue\n",
    "    for bkg in bkg_grps[grp]:\n",
    "        if grp in cutFlowInclGrp2.keys():\n",
    "            try:\n",
    "                cutFlowInclGrp2[grp] += (histos['cutflow_incl'][bkg]*bkgs[bkg]['weight']).astype(int)\n",
    "            except KeyError: pass\n",
    "        else:\n",
    "            try:\n",
    "                cutFlowInclGrp2[grp] = (histos['cutflow_incl'][bkg]*bkgs[bkg]['weight']).astype(int)\n",
    "            except KeyError: pass\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(cutFlowInclGrp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
